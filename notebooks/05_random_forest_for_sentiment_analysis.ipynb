{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T14:39:41.813091Z",
     "start_time": "2019-12-03T12:00:35.808778Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%run 02_data_preparation.ipynb\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "N_ESTIMATORS = (5, 10, 25, 50, 100)\n",
    "CRITERION = (\"gini\", \"entropy\")\n",
    "MAX_FEATURES = (\"auto\", \"log2\", None)\n",
    "\n",
    "# Divide the dataset into train and test fraction\n",
    "train_messages, test_messages, train_targets, test_targets = train_test_split(tweets[\"text\"], \n",
    "                                                                              tweets[\"sentiment\"],\n",
    "                                                                              test_size=0.2)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "for n_estimators, criterion, max_features in itertools.product(N_ESTIMATORS,\n",
    "                                                               CRITERION,\n",
    "                                                               MAX_FEATURES):\n",
    "    # Define the classifier instance\n",
    "    classifier = RandomForestClassifier(random_state=2018, \n",
    "                                        n_estimators=n_estimators, \n",
    "                                        criterion=criterion, \n",
    "                                        max_features=max_features)\n",
    "    # Vectorize preprocessed sentences\n",
    "    train_features = vectorizer.fit_transform(train_messages)\n",
    "\n",
    "    # Train the model\n",
    "    %time fit = classifier.fit(train_features.toarray(), train_targets)\n",
    "\n",
    "    # Check the accuracy of the model on test data and display it\n",
    "    test_features = vectorizer.transform(test_messages)\n",
    "    train_predictions = fit.predict(train_features.toarray())\n",
    "    train_accuracy = accuracy_score(train_predictions, train_targets)\n",
    "    test_predictions = fit.predict(test_features.toarray())\n",
    "    test_accuracy = accuracy_score(test_predictions, test_targets)\n",
    "    print(\"Configuration: n_estimators = {}, criterion = {}, max_features = {}\\n\"\n",
    "          \"Train accuracy score: {}\\n\"\n",
    "          \"Test accuracy score: {}\\n\".format(n_estimators, criterion, max_features, \n",
    "                                             train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turned out the following configuration achieves the best accuracy on our test dataset:\n",
    "\n",
    "`n_estimators = 100, criterion = entropy, max_features = auto`\n",
    "\n",
    "For that reason we are going to create a simple application that will use these parameters for training. That will be a console application reading the sentences from the user and classifies its sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise\n",
    "\n",
    "Please fill the gaps in the source code in *exercise/exercise_03.py*, in order to create an application that will:\n",
    "\n",
    "- create Random Forest based model and train it on a whole dataset used in previous examples, using selected parameters (`n_estimators = 100, criterion = entropy, max_features = auto`)\n",
    "- continiously read the sentences from the standard input and classify them with the created model, until \"exit\" sentence is passed\n",
    "- display the probabilities of beloning to each class\n",
    "\n",
    "Some of the functionalities are already prepared - please complete the source code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest for sentiment analysis\n",
    "\n",
    "In the previous chapter we saw one of the most promising models was based on Random Forest. It combines quite a good accuracy with a performance. It is really important if we'd like to move our model to production, so the best architecture, in terms of its precision, is not always a possible choice. \n",
    "\n",
    "We are going to begin with a simple description of the Random Forest, to understand how it works under the hood. In simple words, Random Forest is a collection of ensembled Decision Trees which vote in order to form a single decision of belonging to a particular class. Each Decision Tree uses a randomly selected subset of features in order to perform its own decision.\n",
    "\n",
    "![Random Forest architecture](images/random-forest.png)\n",
    "\n",
    "## Tuning the model parameters\n",
    "\n",
    "Let's play a little bit with the hyperparameters of the Random Forest Classifier, to optimize its accuracy. First of all, let's list some of the possible parameters and their values: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "From our perspective the following parameters look like the ones we should test out:\n",
    "\n",
    "- **n_estimators** - number of decision trees used to make a forest, by default set to 10\n",
    "- **criterion** - quality function for measuring a split, can be set to \"gini\" (default) or \"entropy\"\n",
    "- **max_features** - a maximum number of features to consider (int - exact number, float - percentage, \"auto\", \"sqrt\", \"log2\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run exercise/exercise_03.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "\n",
    "One of the biggest advantages of Random Forest classifier is the ability to describe the importance of the used features. It allows to check which variables have the best predictive force and to understand how the model performs the decision. The following code snippet visualizes the feature importance for our created model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:37:01.763855Z",
     "start_time": "2019-12-03T15:37:01.227171Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "feature_importances = pd.DataFrame(classifier.feature_importances_, \n",
    "                                   index=vectorizer.get_feature_names(),\n",
    "                                   columns=(\"importance\", ))\n",
    "feature_importances = feature_importances.sort_values(\"importance\", \n",
    "                                                      ascending=False)\n",
    "feature_importances.head(n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T15:37:22.131614Z",
     "start_time": "2019-12-03T15:37:22.076689Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_importances.tail(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
